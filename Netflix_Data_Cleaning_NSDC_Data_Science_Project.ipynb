{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TntJ6WXFZPdo"
      },
      "source": [
        "\n",
        "\n",
        "<h1 align=\"center\">\n",
        "    NSDC Data Science Projects\n",
        "</h1>\n",
        "\n",
        "<h2 align=\"center\">\n",
        "    Project: Netflix Data Cleaning\n",
        "</h2>\n",
        "\n",
        "<h3 align=\"center\">\n",
        "    Name: (insert your name here)\n",
        "</h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKnVUiwQZPdq"
      },
      "source": [
        "### **Please read before you begin your project**\n",
        "\n",
        "**Instructions: Google Colab Notebooks:**\n",
        "\n",
        "Google Colab is a free cloud service. It is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources. We will be using Google Colab for this project.\n",
        "\n",
        " In order to work within the Google Colab Notebook, **please start by clicking on \"File\" and then \"Save a copy in Drive.\"** This will save a copy of the notebook in your personal Google Drive.\n",
        "\n",
        "Please rename the file to \"DSP - Netflix Data Cleaning - Your Full Name.\" Once this project is completed, you will be prompted to share your file with the National Student Data Corps (NSDC) Project Leaders.\n",
        "\n",
        "You can now start working on the project. :)\n",
        "\n",
        "**Project Description:**\n",
        "\n",
        "This project will introduce students to an array of skills as they strive to access and prepare data for further analysis, a process referred to as data cleaning. Whenever data scientists work with any dataset, they must complete this process first to ensure the data is in a suitable format. In this project, students will be able to learn the process and apply it to a Netflix dataset. You should be able to apply this same process to all future datasets you would like to use for data science analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Use this link to join the NSDC DSP Slack Channel!](https://bit.ly/nsdc-dsp-movie-reviews)"
      ],
      "metadata": {
        "id": "xS9ba7_n0aKd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mty7kOOZPdq"
      },
      "source": [
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQpujHnvZPdr"
      },
      "source": [
        "\n",
        "\n",
        "<h3 align = \"center\">\n",
        "    Milestone #1\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e417MmhSZPdr"
      },
      "source": [
        "NOTE: These steps are to be completed **individually**, not as a team. You are encouraged to discuss steps with your teammates. Please attend Office Hours or ask your questions on Slack.\n",
        "\n",
        "GOAL: The main goal of this milestone is to set up your environment, install the required packages, learn how to acces data and do some basic exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmouMnaaZPdr"
      },
      "source": [
        "**Step 1:**\n",
        "\n",
        "Setting up libraries and installing packages\n",
        "\n",
        "To install a library:\n",
        "```python\n",
        " import <library> as <shortname>\n",
        "```\n",
        "We use a *short name* since it is easier to refer to the package to access functions and also to refer to subpackages within the library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le2dKHwnZPds"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy1yhZqNZPdt"
      },
      "source": [
        "These are the libraries that will help us throughout this project. Here is the links to documentation for [Pandas](https://pandas.pydata.org/docs/) and [Numpy](https://numpy.org/doc/) that you can reference if you need help throughout the project as well.\n",
        "\n",
        "We encourage you to read more about the important and most commonly used packages like Pandas and write a few lines in your own words about what they do. [You may use the Data Science Resource Repository (DSRR) to find resources to get started!](https://nebigdatahub.org/nsdc/data-science-resource-repository/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGftlCsOZPdt"
      },
      "source": [
        "<h4 style=\"color:orange\">\n",
        "    TO-DO\n",
        "</h4>\n",
        "\n",
        "Write a few lines about what each library does.\n",
        "\n",
        "- **Pandas:**\n",
        "\n",
        "- **NumPy:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dInyzwncZPdt"
      },
      "source": [
        "**Step 2:**\n",
        "\n",
        "Letâ€™s access our data. We will be using the Netflix Dataset from Kaggle. The dataset contains Netflix media and respective information about each of those movies and TV shows.\n",
        "\n",
        "\n",
        "[The dataset is available at this link](https://www.kaggle.com/datasets/shivamb/netflix-shows). It is better to use the link provided directly within the read_csv function.\n",
        "\n",
        "In order to utilize this dataset, you will have to download the dataset to your computer, unzip it, and upload it to the 'Files' tab of the Google Colab, which can be found on the left banner of the page. In order to access the Files tab, you must connect to a Runtime first. If you are unsure on how to do this, you can refer to this [YouTube video](https://www.youtube.com/watch?v=6HFlwqK3oeo) that will walk you through the steps.\n",
        "\n",
        "\n",
        "We will use pandas to read the data from the csv file using the `read_csv` function. This function returns a pandas dataframe. We will store this dataframe in a variable called `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBDt0Qvzndex"
      },
      "outputs": [],
      "source": [
        "# TODO: Read the data using pandas read_csv function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhFDTTsSZPd4"
      },
      "source": [
        "**Step 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BRrwQ2yZPd4"
      },
      "source": [
        "Let's see what the data looks like. We can use the `head` function which returns the first 5 rows of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24duiI4RpNDN"
      },
      "outputs": [],
      "source": [
        "# TODO: Print the first 5 rows of the data using head function of pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnZloVlvZPd4"
      },
      "source": [
        "There are 12 columns in the dataframe.\n",
        "\n",
        "The `describe()` function gives us a summary of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfzpsKR2BIFy"
      },
      "outputs": [],
      "source": [
        "# TODO: Describe the data using describe function of pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXmADc3X28GM"
      },
      "source": [
        "Why does the `describe()` function only return a summary of 1 out of 12 of the columns?\n",
        "In dataframes, there are different types of data that a column can store. Review those types on this [website](https://pbpython.com/pandas_dtypes.html).\n",
        "\n",
        "Let's look at what data types each of these columns are storing using pandas' dtypes function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG64aqgQ3nyZ"
      },
      "outputs": [],
      "source": [
        "# TODO: Observe the types of data in each column using the dtypes function of pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT9zPK-tZPd5"
      },
      "source": [
        "As you can see, there is only one numerical column, which is why the `describe()` function only returned information for one column. All of the other columns contain the `object` type, which is Python's version of a string or mixed variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7X5T79mZPd6"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3NohCaoZPd6"
      },
      "source": [
        "\n",
        "\n",
        "<h3 align = \"center\">\n",
        "    Milestone #2\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9hFI6uKZPd6"
      },
      "source": [
        "NOTE: These steps are to be completed **individually**, not as a team. You are encouraged to discuss steps with your teammates. Please attend Office Hours or ask your questions on Slack.\n",
        "\n",
        "GOAL: The main goal of this milestone is to clean this dataset, so it is a format suitable for further data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1uBbf33ZPd6"
      },
      "source": [
        "**Step 1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtYBV0jQZPd6"
      },
      "source": [
        "The first step is to check if there are duplicate rows in the dataset and remove them. We can do that using the `duplicated()` function on the show_id column since that is a unique identifier. If two rows have the same show_id, then they are duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NFQtgeAZPd6"
      },
      "outputs": [],
      "source": [
        "# Use the duplicated() function to return rows with the same show_id value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are no rows returned, there are no duplicates present in our dataset, and we can move on to the next step."
      ],
      "metadata": {
        "id": "su-Q1nNd7XSt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAZvcYB47gzs"
      },
      "source": [
        "**Step 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C10AMST-70Lp"
      },
      "source": [
        "The next step is to check if there are any null values in your dataset. To do this, we can use the `isnull()` and `sum()` functions to count how many null values are present in each column of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTsTTjVy70Lq"
      },
      "outputs": [],
      "source": [
        "# Use the isnull() and sum() functions to return count of null values for each column in dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8x-LZRw9F9_"
      },
      "source": [
        "As you can see, there are 6 columns that have null values. We must address each of these null values before moving forward. There are multiple ways to deal with null values, and you must choose which method you will proceed with based on the number of null values and the necessity of that column's data for your future analysis.\n",
        "\n",
        "If that column is integral to your future analysis or there are a lot of null values, then you will want to figure out how to fill the values because you will lose a lot of valuable data if you remove those rows entirely. However, if that column is not important to you, and there are only a few null values, then you can go ahead and remove those rows from the dataset.\n",
        "\n",
        "Here is [an article](https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/) that discusses different ways to address null values. We are going to first drop rows that have null values for columns with a low number of nulls."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the dropna() function and the 'subset' hyperparameter to remove rows in the columns that have 10 or less null value\n",
        "\n",
        "# Use the reset_index() function to reset the dataframe's index\n",
        "\n",
        "# Check how many null values each column has again\n"
      ],
      "metadata": {
        "id": "LA7XSpagixnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now address the columns with large numbers of null values: `director`, `cast`, and `country`. Because there are too many values for us to find information for manually, we would lose a lot of data if we were to delete all of the corresponding rows, and we are unable to use numerical methods to fill in the data with a mean or median value, we will create new categories for these rows.\n",
        "\n",
        "We can use the NumPy library to identify the nan or null values and the`replace()` function to replace the values with a string this. Remember to set the inplace hyperparamter to `True` so the values in the dataframe are replaced permanently!"
      ],
      "metadata": {
        "id": "iR52YtlNlsIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the replace function to replace the NA values in each of these columns with a new identifying string value\n"
      ],
      "metadata": {
        "id": "nKvdG9SOoR5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwoTC0gera1C"
      },
      "source": [
        "**Step 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For columns that aren't numerical but will possibly have a small number of entires, there are some further checks. An example of this is the `type` column because there are only a few possibilities of what the type of media could be, but when analyzing the data, python would consider \"movie\" and \"Movie\" as two different values, so we need to look at what values are currently present and make them all consistent."
      ],
      "metadata": {
        "id": "M3QNUDfnpmD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will use the `unique()` function to list all the current unique values in the column."
      ],
      "metadata": {
        "id": "cQtlD4grp-sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the unique() function to print unique values in the type column\n"
      ],
      "metadata": {
        "id": "dJxGYF0Kp90c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs in this column all have the same capitalization, so we do not need to make any changes. We will check similar columns, such as `rating` as well to make sure there are no necessary changes."
      ],
      "metadata": {
        "id": "PvXEdunmqnlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the unique() function to print unique values in the rating column\n"
      ],
      "metadata": {
        "id": "hf17FMg8qz7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8iM6BY1rcip"
      },
      "source": [
        "**Step 4:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to do make the columns easier to work with for analysis, it is vital to change the type of some of the columns from an object to a type of variable that is easier to analyze. For example, the date added will be much easier to use as a date variable, so let's change it!"
      ],
      "metadata": {
        "id": "6W19IgF7rdvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, return the contents of each column to see what the format is of the data stored."
      ],
      "metadata": {
        "id": "SMBI5qa8yabG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return the date added column\n"
      ],
      "metadata": {
        "id": "d8RcA--hygQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now convert these into date objects. You can convert the `date_added` column to a datetime object using the `to_datetime()` function."
      ],
      "metadata": {
        "id": "DznpC49zytuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the date_added column to a datetime column using to_datetime\n",
        "\n",
        "# Check the current types of the columns to see if it changed\n"
      ],
      "metadata": {
        "id": "9esyVlelzA1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT8Rpdt4ZPeA"
      },
      "source": [
        "<h3 align = 'center' >\n",
        "Your data is now ready for analysis! Thank you for completing the project!\n",
        "</h3>\n",
        "\n",
        "We will have future projects to walk you through how to analyze this cleaned data in python and tableau! Please check those out to continue to grow your data science skills!\n",
        "\n",
        "Please do reach out to us if you have any questions or concerns. We are here to help you learn and grow.\n",
        "\n",
        "If you have any queries, please contact the NSDC HQ Team at nsdc@nebigdatahub.org.\n"
      ]
    }
  ]
}